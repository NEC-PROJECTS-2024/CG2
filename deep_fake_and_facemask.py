# -*- coding: utf-8 -*-
"""DEEP fake and facemask

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fjYLv3116xxLkZ8Rhc84YMYvQ-ib3alT
"""

import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt # image plotting
import os
import time

from tensorflow import keras
import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os

from google.colab import drive
drive.mount('/content/drive')

import os
import random
import shutil

DATA_FOLDER = '/content/drive/MyDrive/4_2 project/deepfake'
TRAIN_SAMPLE_FOLDER = 'dfdc_train_part_48'  # Update to the correct folder name
TRAIN_FOLDER = 'train'
TEST_FOLDER = 'test_videos'

# Set the random seed for reproducibility
random.seed(42)

# Path to the train and test folders
train_folder = os.path.join(DATA_FOLDER, TRAIN_FOLDER)
test_folder = os.path.join(DATA_FOLDER, 'test')

# Create train and test folders if they don't exist
os.makedirs(train_folder, exist_ok=True)
os.makedirs(test_folder, exist_ok=True)

# List all video files in the train sample folder
video_files = os.listdir(os.path.join(DATA_FOLDER,TRAIN_SAMPLE_FOLDER))

# Define the percentage split for train and test
split_percentage = 0.8  # 80% train, 20% test

# Calculate the number of files for the train set
num_train_files = int(len(video_files) * split_percentage)

# Randomly shuffle the list of video files
random.shuffle(video_files)

# Move files to train folder
for file_name in video_files[:num_train_files]:
    source_path = os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, file_name)
    destination_path = os.path.join(train_folder, file_name)
    shutil.move(source_path, destination_path)

# Move remaining files to test folder
for file_name in video_files[num_train_files:]:
    source_path = os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, file_name)
    destination_path = os.path.join(test_folder, file_name)
    shutil.move(source_path, destination_path)

print(f"Number of train samples: {len(os.listdir(train_folder))}")
print(f"Number of test samples: {len(os.listdir(test_folder))}")

train_sample_metadata = pd.read_json('/content/drive/MyDrive/4_2 project/deepfake/test/metadata.json',orient='index')
train_sample_metadata.head()

# Transpose DataFrame to have columns as 'label', 'split', and 'original'
train_sample_metadata = train_sample_metadata.transpose()

# Count the occurrences of each label
label_counts = train_sample_metadata.loc['label'].value_counts()

# Plot the counts as a bar plot
label_counts.plot(kind='bar', figsize=(5, 5), title='The Label in the Training Set')

# Show the plot
plt.show()

# Transpose DataFrame to have columns as 'label', 'split', and 'original'
train_sample_metadata = train_sample_metadata.transpose()
train_sample_metadata.shape

print(train_sample_metadata.columns)

def capture_image_from_video(video_path):
    capture_image = cv2.VideoCapture(video_path)
    ret, frame = capture_image.read()
    fig = plt.figure(figsize =(10,10))
    ax = fig.add_subplot(111)
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    ax.imshow(frame)

import cv2
import matplotlib.pyplot as plt

def capture_image_from_video(video_path):
    cap = cv2.VideoCapture(video_path)

    # Check if the video capture object is opened successfully
    if not cap.isOpened():
        print(f"Error: Could not open video file '{video_path}'")
        return

    try:
        while cap.isOpened():
            ret, frame = cap.read()

            # Break the loop if the video has ended
            if not ret:
                print(f"Video capture ended for '{video_path}'")
                break

            # Check if the frame is empty
            if frame is None:
                print(f"Error: Empty frame received for '{video_path}'")
                continue

            # Convert the frame to RGB
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # Display the frame (you can modify this part based on your needs)
            fig = plt.figure(figsize=(10, 10))
            ax = fig.add_subplot(111)
            ax.imshow(frame)
            plt.show()

            # You can perform additional processing on the frame here

    finally:
        # Release the video capture object when done
        cap.release()

# Example usage:
video_file_path = os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, '/content/drive/MyDrive/4_2 project/deepfake/train/v1.mp4')
capture_image_from_video(video_file_path)

import random

if len(train_sample_metadata.index) >= 5:
    r_train_sample_video = random.sample(list(train_sample_metadata.index), 5)
    print(r_train_sample_video)
else:
    print("Not enough items in the index to sample 5 videos.")

f_videos = list(train_sample_metadata.loc[train_sample_metadata.label=='FAKE'].index)

from IPython.display import HTML
from base64 import b64encode

TRAIN_SAMPLE_FOLDER = 'train'

def play_video(video_file, subset=TRAIN_SAMPLE_FOLDER,video_list=[]):
    # Read the video file
    video_path = os.path.join(DATA_FOLDER, subset, video_file)
    video_url = open(video_path, 'rb').read()#Reads the content of the video file in binary mode and stores it in the variable video_url.

    # Convert the video to a base64-encoded data URL
    data_url = "data:video/mp4;base64," + b64encode(video_url).decode()

    # Display the video using the HTML5 <video> tag with playback controls
    return HTML("""<video width=500 controls><source src="%s" type="video/mp4"></video>""" % data_url)

# Example usage:
f_videos = ['v1.mp4','v2.mp4','v3.mp4','v4.mp4','v5.mp4']
play_video(f_videos[2])
play_video(video_file=f_videos[2], video_list=f_videos)

img_size = 224
batch_size = 64
epochs = 15

max_seq_length = 20
num_features = 2048

def crop_center_square(frame):
    y,x = frame.shape[0:2]
    min_dim = min(y, x)
    start_x = (x // 2) - (min_dim // 2)
    start_y = (y // 2) - (min_dim // 2)
    return frame[start_y :start_y + min_dim, start_x : start_x + min_dim]

def load_video(path, max_frames=0, resize=(img_size, img_size)):
    cap = cv2.VideoCapture(path)
    frames = []
    try:
        while 1:
            ret, frame = cap.read()
            if not ret:
                break
            frame = crop_center_square(frame)
            frame = cv2.resize(frame, resize)
            frame = frame[:, :, [2, 1, 0]]
            frames.append(frame)

            if len(frames) == max_frames:
                break
    finally:
        cap.release()
    return np.array(frames)

def pretrain_feature_extractor():
    feature_extractor = keras.applications.InceptionV3(
    weights = "imagenet",
    include_top=False,
    pooling="avg",
    input_shape = (img_size,img_size,3)
    )
    preprocess_input = keras.applications.inception_v3.preprocess_input

    inputs = keras.Input((img_size,img_size,3))
    preprocessed = preprocess_input(inputs)

    outputs = feature_extractor(preprocessed)
    return keras.Model(inputs, outputs, name="feature_extractor")

feature_extractor = pretrain_feature_extractor()

def prepare_all_videos(df,root_dir): #df是train_sample_metadata->json的split
    num_samples = len(df)
    video_paths = list(df.index)
    labels = df["label"].values
    labels = np.array(labels=='FAKE').astype(int)

    frame_masks = np.zeros(shape=(num_samples, max_seq_length), dtype="bool") #array=360*20
    frame_features = np.zeros(
        shape=(num_samples, max_seq_length, num_features), dtype="float32" #array=360*20*2048
    )

    for idx, path in enumerate(video_paths):
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        temp_frame_mask = np.zeros(shape=(1, max_seq_length,), dtype="bool")
        temp_frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype="float32")

        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(max_seq_length, video_length) #if length is over 20s ,only cut 20s
            for j in range(length):
                temp_frame_features[i, j, :] =feature_extractor.predict(batch[None, j, :])
            temp_frame_mask[i, :length] =1 # 1 = not masked, 0 = masked ->give 1 when there are images ,otherwise 0 for padding

        frame_features[idx,] =temp_frame_features.squeeze() #squeeze array for training
        frame_masks[idx,] =temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels

def prepare_all_videos(df,root_dir): #df是train_sample_metadata->json的split
    num_samples = len(df)
    video_paths = list(df.index)
    labels = df["label"].values
    labels = np.array(labels=='FAKE').astype(int)

    frame_masks = np.zeros(shape=(num_samples, max_seq_length), dtype="bool") #array=360*20
    frame_features = np.zeros(
        shape=(num_samples, max_seq_length, num_features), dtype="float32" #array=360*20*2048
    )

    for idx, path in enumerate(video_paths):
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        temp_frame_mask = np.zeros(shape=(1, max_seq_length,), dtype="bool")
        temp_frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype="float32")

        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(max_seq_length, video_length) #if length is over 20s ,only cut 20s
            for j in range(length):
                temp_frame_features[i, j, :] =feature_extractor.predict(batch[None, j, :])
            temp_frame_mask[i, :length] =1 # 1 = not masked, 0 = masked ->give 1 when there are images ,otherwise 0 for padding

        frame_features[idx,] =temp_frame_features.squeeze() #squeeze array for training
        frame_masks[idx,] =temp_frame_mask.squeeze()

from sklearn.model_selection import train_test_split

# Assuming 'label' is the column in train_sample_metadata that you want to stratify by
#stratify_train_metadata = train_sample_metadata['lbvpjkfemg.mp4']

Train_set, Test_set = train_test_split(
    train_sample_metadata,
    test_size=0.1,
    random_state=42,

)

print("Training set shape:", Train_set.shape)
print("Test set shape:", Test_set.shape)

from sklearn.model_selection import train_test_split
Train_set, Test_set = train_test_split(train_sample_metadata, test_size=0.1, random_state=42)
print(Train_set.shape, Test_set.shape)

from sklearn.model_selection import train_test_split
Train_set, Test_set = train_test_split(train_sample_metadata, test_size=0.1, random_state=42)
print(Train_set.shape, Test_set.shape)

# Assuming 'label' is the column in train_sample_metadata that you want to stratify by
stratify_train_metadata = train_sample_metadata['label']


Train_set, Test_set = train_test_split(
    train_sample_metadata,
    test_size=0.1,
    random_state=42,
    stratify=stratify_train_metadata
)

print("Training set shape:", Train_set.shape)
print("Test set shape:", Test_set.shape)

print(type(Train_set['label']))

train_data,train_labels = prepare_all_videos(Train_set,'train')
test_data,test_labels = prepare_all_videos(Test_set,'test')

print(f"Frame features in train set:{train_data[0].shape}")
print(f"Frame masks in train set:{train_data[1].shape}")

frame_features_input = keras.Input((max_seq_length, num_features))
mask_input = keras.Input((max_seq_length,),dtype="bool")

x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask = mask_input)
x = keras.layers.GRU(8)(x)
x = keras.layers.Dropout(0.4)(x)
x = keras.layers.Dense(8, activation="relu")(x)
output = keras.layers.Dense(1, activation="sigmoid")(x)

model = keras.Model([frame_features_input, mask_input], output)
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
model.summary()

checkpoint = keras.callbacks.ModelCheckpoint('./', save_weights_only=True, save_best_only=True)
history = model.fit(
        [train_data[0], train_data[1]],
        train_labels,
        validation_data=([test_data[0], test_data[1]], test_labels),
        callbacks=[checkpoint],
        epochs=epochs,
        batch_size=8
)

TEST_FOLDER='/content/drive/MyDrive/4_2 project/deepfake/test'
test_videos = pd.DataFrame(list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER))), columns=['video'])

def prepare_single_video(frames):
    frames = frames[None, ...]
    frame_mask = np.zeros(shape=(1, max_seq_length,), dtype="bool")
    frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype="float32")

    for i, batch in enumerate(frames):
        video_length = batch.shape[0]
        length = min(max_seq_length, video_length)
        for j in range(length):
            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])
        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

    return frame_features, frame_mask

def sequence_prediction(path):
    frames = load_video(os.path.join(DATA_FOLDER, TEST_FOLDER,path))
    frame_features, frame_mask = prepare_single_video(frames)
    return model.predict([frame_features, frame_mask])[0]

# This utility is for visualization.
# Referenced from:
# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub
def to_gif(images):
    converted_images = images.astype(np.uint8)
    imageio.mimsave("animation.gif", converted_images, fps=10)
    return embed.embed_file("animation.gif")


test_video = np.random.choice(test_videos["video"].values.tolist())
print(f"Test video path: {test_video}")

if(sequence_prediction(test_video)>=0.5):
    print(f'The predicted class of the video is FAKE')
else:
    print(f'The predicted class of the video is REAL')

play_video(test_video,TEST_FOLDER)

input_data_path = '/content/drive/MyDrive/face/images'
annotations_path = "/content/drive/MyDrive/face/annotations"
images = [*os.listdir("/content/drive/MyDrive/face/images")]
output_data_path =  '.'

import xml.etree.ElementTree as ET # is used to parse an XML (inherently hierarchical) data format, which is the format of the annotations file

def parse_annotation(path):
    tree = ET.parse(path)
    root = tree.getroot()
    constants = {}
    objects = [child for child in root if child.tag == 'object']
    for element in tree.iter():
        if element.tag == 'filename':
            constants['file'] = element.text[0:-4]
        if element.tag == 'size':
            for dim in list(element):
                if dim.tag == 'width':
                    constants['width'] = int(dim.text)
                if dim.tag == 'height':
                    constants['height'] = int(dim.text)
                if dim.tag == 'depth':
                    constants['depth'] = int(dim.text)
    object_params = [parse_annotation_object(obj) for obj in objects]
    #print(constants)
    full_result = [merge(constants,ob) for ob in object_params]
    return full_result


def parse_annotation_object(annotation_object):
    params = {}
    for param in list(annotation_object):
        if param.tag == 'name':
            params['name'] = param.text
        if param.tag == 'bndbox':
            for coord in list(param):
                if coord.tag == 'xmin':
                    params['xmin'] = int(coord.text)
                if coord.tag == 'ymin':
                    params['ymin'] = int(coord.text)
                if coord.tag == 'xmax':
                    params['xmax'] = int(coord.text)
                if coord.tag == 'ymax':
                    params['ymax'] = int(coord.text)

    return params

def merge(dict1, dict2):
    res = {**dict1, **dict2}
    return res

import glob
dataset = [parse_annotation(anno) for anno in glob.glob(annotations_path+"/*.xml") ]

# Since the output of the parse_annotation function is a list of lists, we need to flatten the ctopped faces.
# i.e make it a list of images instead of a list of lists.
full_dataset = sum(dataset, []) #
#full_dataset

df = pd.DataFrame(full_dataset)
df.shape

df.head()

final_test_image = 'maksssksksss0' # chose the image
df_final_test = df.loc[df["file"] == final_test_image] # create a separate dataframe which contain only the people in this specific image
images.remove(f'{final_test_image}.png') # remove the image from the full dataset
df = df.loc[df["file"] != final_test_image] # remove the information of the image from the full dataset

df.rename(columns = {'file':'file_name', 'name':'label'}, inplace = True)
df_final_test.rename(columns = {'file':'file_name', 'name':'label'}, inplace = True)#renaming the columns in the data frame

df["label"].value_counts()

df["label"].value_counts().plot(kind='barh')
plt.xlabel('Count', fontsize = 10, fontweight = 'bold')
plt.ylabel('Label', fontsize = 10, fontweight = 'bold')

labels = df['label'].unique()
directory = ['train', 'test', 'val']
output_data_path =  '.'

import os
for label in labels:
    for d in directory:
        path = os.path.join(output_data_path, d, label)
        #print(path)
        if not os.path.exists(path):
            os.makedirs(path)

from PIL import Image
def crop_img(image_path, x_min, y_min, x_max, y_max):

    '''
     This function takes an image path + x and y coordinates of two opposite corners of the rectangle
     and returns a cropped image
    '''
    x_shift = (x_max - x_min) * 0.1
    y_shift = (y_max - y_min) * 0.1
    img = Image.open(image_path)
    cropped = img.crop((x_min - x_shift, y_min - y_shift, x_max + x_shift, y_max + y_shift))
    return cropped

def extract_faces(image_name, image_info):

    '''
     This function takes an image name + dataframe with information about the image
     and splits the image into all the different faces. image name contains the
     upper-left coordinate of each face so we could distinguish it later
    '''
    faces = []
    df_one_img = image_info[image_info['file_name'] == image_name[:-4]][['xmin', 'ymin', 'xmax', 'ymax', 'label']]
    #print(df_one_img)
    for row_num in range(len(df_one_img)):
        x_min, y_min, x_max, y_max, label = df_one_img.iloc[row_num]
        image_path = os.path.join(input_data_path, image_name)
        faces.append((crop_img(image_path, x_min, y_min, x_max, y_max), label,f'{image_name[:-4]}_{(x_min, y_min)}'))
    return faces

cropped_faces = [extract_faces(img, df) for img in images]

#flat_cropped_faces = [item for sublist in cropped_faces for item in sublist]
flat_cropped_faces = sum(cropped_faces, [])
#flat_cropped_faces

with_mask = [(img, image_name) for img, label,image_name in flat_cropped_faces if label == "with_mask"]
mask_weared_incorrect = [(img, image_name) for img, label,image_name in flat_cropped_faces if label == "mask_weared_incorrect"]
without_mask = [(img, image_name) for img, label,image_name in flat_cropped_faces if label == "without_mask"]

print(f'num of images with mask: {len(with_mask)}')
print(f'num of images without mask: {len(without_mask)}')
print(f'num of images incorrect mask: {len(mask_weared_incorrect)}')
print(f'sum: {len(with_mask) + len(without_mask) + len(mask_weared_incorrect) }')

from sklearn.model_selection import train_test_split

train_with_mask, test_with_mask = train_test_split(with_mask, test_size=0.20, random_state=42)
test_with_mask, val_with_mask = train_test_split(test_with_mask, test_size=0.7, random_state=42)

train_mask_weared_incorrect, test_mask_weared_incorrect = train_test_split(mask_weared_incorrect, test_size=0.20, random_state=42)
test_mask_weared_incorrect, val_mask_weared_incorrect = train_test_split(test_mask_weared_incorrect, test_size=0.7, random_state=42)

train_without_mask, test_without_mask = train_test_split(without_mask, test_size=0.20, random_state=42)
test_without_mask, val_without_mask = train_test_split(test_without_mask, test_size=0.7, random_state=42)

def save_image(image, image_name, output_data_path,  dataset_type, label):
    '''
     This function takes an image name + a path of output folder
     and saves image into the output folder
    '''

    output_path = os.path.join(output_data_path, dataset_type, label ,f'{image_name}.png')
    image.save(output_path)

# Train set


for image, image_name in train_with_mask:
    save_image(image, image_name, output_data_path, 'train', 'with_mask')

for image, image_name in train_mask_weared_incorrect:
    save_image(image, image_name, output_data_path, 'train', 'mask_weared_incorrect')

for image, image_name in train_without_mask:
    save_image(image, image_name, output_data_path, 'train', 'without_mask')

# Test set

for image, image_name in test_with_mask:
    save_image(image, image_name, output_data_path, 'test', 'with_mask')

for image, image_name in test_mask_weared_incorrect:
    save_image(image, image_name, output_data_path, 'test', 'mask_weared_incorrect')

for image, image_name in test_without_mask:
    save_image(image, image_name, output_data_path, 'test', 'without_mask')

# Val set

for image, image_name in val_with_mask:
    save_image(image, image_name, output_data_path, 'val', 'with_mask')

for image, image_name in val_without_mask:
    save_image(image, image_name, output_data_path, 'val', 'without_mask')

for image, image_name in val_mask_weared_incorrect:
    save_image(image, image_name, output_data_path, 'val', 'mask_weared_incorrect')

# Defineing Some Constant Parameters
batch_size = 8
epochs = 100

from keras.preprocessing.image import ImageDataGenerator
# class from Keras to perform real-time data augmentation on images and create data generators for training, validation, and testing.
datagen = ImageDataGenerator(
    rescale=1.0 / 255, horizontal_flip=True, zoom_range=0.1, shear_range=0.2, width_shift_range=0.1,
    height_shift_range=0.1, rotation_range=4, vertical_flip=False

)

val_datagen = ImageDataGenerator(
    rescale=1.0 / 255
)


train_generator = datagen.flow_from_directory(
    directory='/content/train',
    target_size = (35,35),
    class_mode="categorical", batch_size=batch_size, shuffle=True

)

# Validation data
val_generator = val_datagen.flow_from_directory(
    directory='/content/val',
    target_size = (35,35),
    class_mode="categorical", batch_size=batch_size, shuffle=True
)

# Test data
test_generator = val_datagen.flow_from_directory(
    directory='/content/test',
    target_size = (35,35),
    class_mode="categorical", batch_size=batch_size, shuffle=False
)

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

model_custom = Sequential()
model_custom.add(Conv2D(filters = 16, kernel_size = 3,  padding='same', activation = 'relu', input_shape = (35,35,3)))
model_custom.add(MaxPooling2D(pool_size = 2))
model_custom.add(Conv2D(filters = 32, kernel_size = 3,  padding='same', activation = 'relu'))
model_custom.add(MaxPooling2D(pool_size = 2))
model_custom.add(Conv2D(filters = 64, kernel_size = 3,  padding='same', activation = 'relu'))
model_custom.add(MaxPooling2D(pool_size = 2))
model_custom.add(Dropout(0.3))
model_custom.add(Flatten())
model_custom.add(Dense(units = 500, activation = 'relu'))
model_custom.add(Dropout(0.3))
model_custom.add(Dense(units = 3, activation = 'softmax'))


model_custom.summary()

data_size = len(train_generator)
#data_size2 = train_generator.n

#print(f"data_size: {data_size}, {data_size2}")

steps_per_epoch = int(data_size / batch_size)
print(f"steps_per_epoch: {steps_per_epoch}")

val_steps = int(len(val_generator) // batch_size)
#print(f"val size: {len(val_generator)}")
print(f"val_steps: {val_steps}")

# Compiling the model
model_custom.compile(
    optimizer="adam",
    loss="categorical_crossentropy",
    metrics=['accuracy', 'Recall', 'Precision', 'AUC']

)

# Early Stopping
# Before training the network, we define an early stopping criterion,
#to avoid redundent epochs once the model has already converged.
from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)

# Reduce Learning Rate On Plateau
# We define a `ReduceLROnPlateau` callback to reduce the learning rate when the metric
# we chose (`val_loss`) has stopped improving.
from tensorflow.keras.callbacks import ReduceLROnPlateau
lrr = ReduceLROnPlateau(monitor='val_loss',patience=8,verbose=1,factor=0.5, min_lr=0.00001)

# Fit the model on train data

model_custom_history = model_custom.fit_generator(
    generator=train_generator,
    steps_per_epoch=steps_per_epoch,
    epochs=epochs,
    shuffle=True,
    #verbose=2,
    validation_data=val_generator,
    validation_steps=val_steps,
    callbacks=[lrr]
)

model_custom.save('model_custom.h5')

# Evaluate model performance on test data
model_custom_loss, model_custom_acc, custom_recall, custom_precision, custom_auc = model_custom.evaluate(test_generator)
print("Our Custom Model has a loss of %.2f and accuracy %.2f%%" % (model_custom_loss, model_custom_acc*100))
print("Our Custom Model has a recall of %.2f%%, precision of %.2f%% and auc of %.2f%%" % (custom_recall*100, custom_precision*100, custom_auc*100))

custom_predictions = model_custom.predict(test_generator)
print("predictions shape:", custom_predictions.shape)

import cv2

paths = test_generator.filenames # Your files path

y_pred_custom = model_custom.predict(test_generator).argmax(axis=1) # Predict prob and get Class Indices
classes = test_generator.class_indices  # Map of Indices to Class name

# Randomly select an image
a_img_rand = np.random.randint(0,len(paths))   # A rand to pick a rand image
img = cv2.imread(os.path.join(output_data_path,'test', paths[a_img_rand]))
colored_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)  # colored output image

#img = image.img_to_array(img)
plt.imshow(colored_img)
true_label = paths[a_img_rand].split('/')[0]
predicted_label = list(classes)[y_pred_custom[a_img_rand]]
print(f'Class Predicted: {predicted_label} , True label: {true_label} for Custom Model')

import cv2
img = cv2.imread(os.path.join(input_data_path, f'{final_test_image}.png'))
colored_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)  # colored output image

#print(type(colored_img))
mask_label = {0:'MASK INCORRECT',1:'MASK', 2:'NO MASK'}
color_label = {0:(0,255,255),1:(0, 255,0), 2:(255,0,0)}
cropped_faces = extract_faces(f'{final_test_image}.png', df_final_test)

df_test_img = df_final_test[['xmin', 'ymin', 'xmax', 'ymax', 'label']]
#df_test_img
#for row_num in range(len(df_test_img)):
for idx, face in enumerate(cropped_faces):
    #print(idx)
    x_min, y_min, x_max, y_max, label = df_test_img.iloc[idx]
    #print(x_min, y_min, x_max, y_max, label)
    #print(face[0])
    resized_face = cv2.resize(np.array(face[0]),(35,35))
    reshaped_face = np.reshape(resized_face,[1,35,35,3])/255.0

    face_result = model_custom.predict(reshaped_face)
    cv2.putText(colored_img,mask_label[face_result.argmax()],(x_min, y_min-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,color_label[face_result.argmax()],2)
    cv2.rectangle(colored_img,(x_min, y_min), (x_max, y_max), color_label[face_result.argmax()]) # print a blue rectangle of each person's face using the given coordinates

plt.figure(figsize=(10, 10))
plt.imshow(colored_img)







